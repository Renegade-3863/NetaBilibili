# -*- coding: utf-8 -*-
"""PA2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16IcSweeZVITLbkoWaYykdfXQw5LWiwNO

## Import necessary libraries for classification models
"""

import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam, SGD, RMSprop
from tensorflow.keras.regularizers import l1, l2
from tensorflow.keras.callbacks import EarlyStopping
# Implementing random forest for designing new models
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
# Trying KNN models
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.ensemble import AdaBoostClassifier, ExtraTreesClassifier

"""## Load the datasets"""

def load_datasets():
  # Load the first dataset (labelled)
  data1 = pd.read_csv("./firstData.txt", header=None, sep='\s+', engine='python')
  X = data1.iloc[:, :-1].values
  y = data1.iloc[:, -1].values
  # print(X)

  # Load the second dataset (unlabelled)
  data2 = pd.read_csv("./secondData.txt", header=None, sep='\s+', engine='python')
  X_test = data2.values

  # Return the pre-processed data back to the caller
  return X, y, X_test

"""## Preprocess the datasets"""

# Prepare a StandardScaler for standardizing the data
# Note:
# This scaler is used for converting the distribution curve from a random Gaussian Distribution
# Into a standart Gaussian Distribution
scaler = StandardScaler()
# Preprocess the datasets, we have non-floating point data columns in the dataset
def preprocess_datasets(X, y=None, countries=None, is_training=True):
  df = pd.DataFrame(X)

  binary_cols = list(range(9))
  age_col = 9
  gender_col = 10
  country_col = 11

  # Averaging the ages, for tag '60+', convert it to 65 instead
  def process_age(age_str):
    if isinstance(age_str, str) and '-' in age_str:
      low, high = map(int, age_str.split('-'))
      return (low + high) / 2
    elif age_str == '60+':
      return 65
    # Maybe not necessary
    else:
      return np.nan
  df['age_numeric'] = df[age_col].apply(process_age)

  # Enumerating the gender strings
  # Note that we have one additional 'Trans_Gender' column
  df['gender_numeric'] = df[gender_col].map({'Female': 0, 'Male': 1, 'Trans_Gender': 2})

  # Please see the algorithm description in the report, the conversion algorithm for the country column is kind of complex
  # We didn't implement the one-hot encoding, instead, we replace the country categorical value into their corresponding frequency within the training dataset
  # This is more scalable when the value range is large
  if is_training:
    # Calculate the number of countries in the dataset
    country_counts = df[country_col].value_counts()
    # Calculate the frequency of each individual country
    country_freq = (country_counts / len(df)).to_dict()
    # Add a new column recording the country frequencies
    df['country_freq'] = df[country_col].map(country_freq)

    # Pass the calculated results back to the caller
    unique_countries = df[country_col].unique()
  else:
    if countries is not None:
      default_freq = 1 / len(countries) if len(countries) > 0 else 0
      country_freq = {country: default_freq for country in countries}
      # get function: index the key x, if no value is found, use default_freq instead
      df['country_freq'] = df[country_col].apply(lambda x: country_freq.get(x, default_freq))

  # Pack the final returned table
  keep_cols = binary_cols + ['age_numeric', 'gender_numeric', 'country_freq']
  X_processed = df[keep_cols].values

  # Standardize the values of inputs
  if is_training:
    X_processed = scaler.fit_transform(X_processed)
  else:
    X_processed = scaler.transform(X_processed)

  if y is not None:
    return X_processed, y, unique_countries if is_training else None
  else:
    return X_processed, unique_countries if is_training else None

"""## Define the models"""

def get_models(input_dim):
  models = {}

  # Note: Since we used quite a few random models.
  # we introduced fixed seed (42) for reproduction of the experiment results

  # Model 1: Random Forest, setting the tree num to be 22
  # tree maximum depth to be 5
  # min sample split num to be 2
  # min sample per leaf to be 1
  # Random seeds are used for reproduction of the results
  model1 = RandomForestClassifier(
      n_estimators=3,
      max_depth=5,
      min_samples_split=2,
      min_samples_leaf=1,
      random_state=42
  )
  models['model1'] = model1

  # Memo: Tried several different models like logisticRegression, GaussianNB, etc, but they kind of giving
  # Similar results, which is not quite good for comparison
  # Note: advantages of adaboost classifier:
  # Compatible with high dimensional data
  # We use 32 sub-decision trees for classification purposes
  # In comparison with the Gradient boosting classifier regarding the performances
  model2 = AdaBoostClassifier(
      n_estimators=32,
      random_state=42
  )
  models['model2'] = model2

  # Model 3: Try a Gradient Boosting Machine
  model3 = GradientBoostingClassifier(
      n_estimators=75,
      learning_rate=0.03,
      max_depth=3,
      random_state=42
  )
  models['model3'] = model3

  # Note: We tried KNN here at the very beginning, but it turns out that KNN is not that robust, so we disgarded it
  # Model 4: Implement a KNN for testing
  # model4 = KNeighborsClassifier(
  #     n_neighbors=3,
  #     weights="distance",
  #     algorithm="auto"
  # )
  # models['model4'] = model4

  # Model 4: A deep neural network with several dropout layers
  model4 = Sequential([
      Dense(32, activation='relu', input_dim=input_dim),
      Dropout(0.3),
      Dense(16, activation='relu'),
      Dropout(0.2),
      Dense(1, activation='sigmoid')
  ])
  model4.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
  models['model4'] = model4

  # Model 5: Finally, give SVM a try
  # It is said that SVM is more suitable for dataset of small size
  # We use it to compare with the more popular and robust deep neural network model (i.e. model 4)
  model5 = SVC(
      kernel='rbf',
      probability=True,
      random_state=42
  )
  models['model5'] = model5

  return models

"""# Train the models and use the models for prediction"""

def train_and_predict(models, X, y, X_test):
  # Split the data for training set and validation set
  # Hard code the random seed to be 42 for reproduction of the experiment
  X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

  predictions = {}
  # Use early_stop to accelerate the training
  early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

  for name, model in models.items():
    print(f'Training {name}...')
    if isinstance(model, Sequential):
      history = model.fit(
          X_train, y_train,
          validation_data=(X_val, y_val),
          epochs=100,
          batch_size=32,
          callbacks=[early_stop],
          verbose=1
      )
      y_pred = (model.predict(X_test) > 0.5).astype(int).flatten()
    else:
      model.fit(X_train, y_train)
      y_pred = model.predict(X_test)

    predictions[name] = y_pred

  return predictions

"""## Save predictions to files"""

def save_predictions(predictions):
  for i in range(1, 6):
    model_name = f"model{i}"
    pred = predictions[model_name]
    np.savetxt(f"predicted{i}.txt", pred, fmt="%d")

"""## Execute the code"""

def main():
  X, y, X_test = load_datasets()

  # When we preprocess the dataset, we don't add the countries (frequency list) var to the parameters list
  # Let the function help us calculate it
  X_preprocessed, y_train, countries = preprocess_datasets(X, y, is_training=True)

  X_test_processed, _ = preprocess_datasets(X_test, countries=countries, is_training=False)

  models = get_models(input_dim=X_preprocessed.shape[1])

  predictions = train_and_predict(models, X_preprocessed, y_train, X_test_processed)

  save_predictions(predictions)

  for i in range(1, 6):
    for j in range(i+1, 6):
      model_name1 = f"model{i}"
      model_name2 = f"model{j}"
      pred1 = predictions[model_name1]
      pred2 = predictions[model_name2]
      print(len(pred1))
      same = 0
      for k in range(0, len(pred1)):
        # print(f"{pred1[k]}, {pred2[k]}")
        if pred1[k] == pred2[k]:
          same += 1
      print(f"model{i} agrees with model{j} in rate: {same/2006}")
if __name__ == "__main__":
  main()